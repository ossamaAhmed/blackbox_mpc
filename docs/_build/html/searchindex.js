Search.setIndex({docnames:["index","modules/dynamics_functions/dynamics_functions","modules/dynamics_handlers/dynamics_handlers","modules/environment_utils/environment_utils","modules/optimizers/optimizers","modules/policies/policies","modules/runner/runner","modules/blackbox_mpc","modules/trajectory_evaluators/trajectory_evaluators"],envversion:{"sphinx.domains.c":1,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":1,"sphinx.domains.index":1,"sphinx.domains.javascript":1,"sphinx.domains.math":2,"sphinx.domains.python":1,"sphinx.domains.rst":1,"sphinx.domains.std":1,"sphinx.ext.viewcode":1,sphinx:56},filenames:["index.rst","modules/dynamics_functions/dynamics_functions.rst","modules/dynamics_handlers/dynamics_handlers.rst","modules/environment_utils/environment_utils.rst","modules/optimizers/optimizers.rst","modules/policies/policies.rst","modules/runner/runner.rst","modules/blackbox_mpc.rst","modules/trajectory_evaluators/trajectory_evaluators.rst"],objects:{"tf_neuralmpc.dynamics_functions":{DeterministicDynamicsFunctionBase:[1,1,1,""],DeterministicMLP:[1,1,1,""]},"tf_neuralmpc.dynamics_functions.DeterministicDynamicsFunctionBase":{__call__:[1,2,1,""],__init__:[1,2,1,""],get_loss:[1,2,1,""],get_validation_loss:[1,2,1,""]},"tf_neuralmpc.dynamics_functions.DeterministicMLP":{__call__:[1,3,1,""],__init__:[1,2,1,""],add_layer:[1,2,1,""],get_loss:[1,3,1,""],get_validation_loss:[1,3,1,""]},"tf_neuralmpc.dynamics_handlers":{SystemDynamicsHandler:[2,1,1,""]},"tf_neuralmpc.dynamics_handlers.SystemDynamicsHandler":{__init__:[2,2,1,""],__weakref__:[2,3,1,""],process_input:[2,2,1,""],process_state_output:[2,2,1,""],train:[2,2,1,""]},"tf_neuralmpc.environment_utils":{EnvironmentWrapper:[3,1,1,""]},"tf_neuralmpc.environment_utils.EnvironmentWrapper":{__weakref__:[3,3,1,""],make_custom_gym_env:[3,2,1,""],make_standard_gym_env:[3,2,1,""]},"tf_neuralmpc.optimizers":{CEMOptimizer:[4,1,1,""],CMAESOptimizer:[4,1,1,""],OptimizerBase:[4,1,1,""],PI2Optimizer:[4,1,1,""],PSOOptimizer:[4,1,1,""],RandomSearchOptimizer:[4,1,1,""],SPSAOptimizer:[4,1,1,""]},"tf_neuralmpc.optimizers.CEMOptimizer":{__call__:[4,3,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.optimizers.CMAESOptimizer":{__call__:[4,3,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.optimizers.OptimizerBase":{__call__:[4,2,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.optimizers.PI2Optimizer":{__call__:[4,3,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.optimizers.PSOOptimizer":{__call__:[4,3,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.optimizers.RandomSearchOptimizer":{__call__:[4,3,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.optimizers.SPSAOptimizer":{__call__:[4,3,1,""],__init__:[4,2,1,""],reset:[4,2,1,""]},"tf_neuralmpc.policies":{MPCPolicy:[5,1,1,""],ModelBasedBasePolicy:[5,1,1,""],ModelFreeBasePolicy:[5,1,1,""],RandomPolicy:[5,1,1,""]},"tf_neuralmpc.policies.MPCPolicy":{__init__:[5,2,1,""],act:[5,2,1,""],reset:[5,2,1,""],switch_optimizer:[5,2,1,""]},"tf_neuralmpc.policies.ModelBasedBasePolicy":{__init__:[5,2,1,""],__weakref__:[5,3,1,""],act:[5,2,1,""],reset:[5,2,1,""]},"tf_neuralmpc.policies.ModelFreeBasePolicy":{__init__:[5,2,1,""],__weakref__:[5,3,1,""],act:[5,2,1,""],reset:[5,2,1,""]},"tf_neuralmpc.policies.RandomPolicy":{__init__:[5,2,1,""],act:[5,2,1,""],reset:[5,2,1,""]},"tf_neuralmpc.runner":{Runner:[6,1,1,""]},"tf_neuralmpc.runner.Runner":{__init__:[6,2,1,""],__weakref__:[6,3,1,""],learn_dynamics_from_randomness:[6,2,1,""],learn_dynamics_iteratively_w_mpc:[6,2,1,""],make_mpc_policy:[6,2,1,""],perform_rollouts:[6,2,1,""],record_rollout:[6,2,1,""],sample:[6,2,1,""]},"tf_neuralmpc.trajectory_evaluators":{DeterministicTrajectoryEvaluator:[8,1,1,""],EvaluatorBase:[8,1,1,""]},"tf_neuralmpc.trajectory_evaluators.DeterministicTrajectoryEvaluator":{__call__:[8,3,1,""],__init__:[8,2,1,""],evaluate_next_reward:[8,2,1,""],predict_next_state:[8,2,1,""]},"tf_neuralmpc.trajectory_evaluators.EvaluatorBase":{__call__:[8,3,1,""],__init__:[8,2,1,""],evaluate_next_reward:[8,2,1,""],predict_next_state:[8,2,1,""]},tf_neuralmpc:{dynamics_functions:[1,0,0,"-"],dynamics_handlers:[2,0,0,"-"],environment_utils:[3,0,0,"-"],optimizers:[4,0,0,"-"],policies:[5,0,0,"-"],runner:[6,0,0,"-"],trajectory_evaluators:[8,0,0,"-"]}},objnames:{"0":["py","module","Python module"],"1":["py","class","Python class"],"2":["py","method","Python method"],"3":["py","attribute","Python attribute"]},objtypes:{"0":"py:module","1":"py:class","2":"py:method","3":"py:attribute"},terms:{"150ga":4,"1xdim_u":[4,5],"20particl":4,"20swarm":4,"abstract":0,"boolean":6,"case":2,"class":[0,1,2,3,4,5,6,7],"default":[2,4],"float":[2,6],"function":[0,2,3,4,5,6,7,8],"import":0,"int":[1,2,3,4,6],"new":1,"return":[1,2,3,4,5,6,8],"static":3,"switch":5,"true":[0,2,5,6],"while":6,The:[0,2,4,5,6,8],There:0,These:0,__call__:[1,4,8],__init__:[1,2,4,5,6,8],__weakref__:[2,3,5,6],_reading6:4,a_par:4,a_t:1,abl:[],absolut:2,abund:0,accept:[],achiev:4,acquir:[],act:[0,5],action:[1,2,4,5,6,8],action_lower_bound:[4,5],action_sequ:8,action_to_execut:0,action_upper_bound:[4,5],actions_reward_funct:[0,6,8],actions_trajectori:2,activ:1,activation_funct:1,adam:[2,6],adapt:[0,7],add:1,add_lay:1,added:[4,5,6],advantag:0,after:[2,4],agent:[0,2,3,4,5,6],agnost:[],ahm:[],aim:0,algorithm:0,alia:[],all:6,alpha:4,alpha_cov:4,ani:0,api:[],appli:[4,5,8],approxim:[0,7],arnumb:4,arxiv:4,author:[],base:[0,7],batch:[1,2,6],batch_siz:[2,6],batchxdim_:[6,8],batchxdim_u:[6,8],befor:4,begin:[4,5],being:6,below:0,best:[4,5,6],between:[4,5,6],bia:[],block:[1,4,8],bool:[1,2,4,5,6],bound:[4,5],calcul:[4,8],call:[1,2,4,5,6,8],can:0,capac:0,care:3,cem:[4,5,6],cemoptim:4,cetutori:4,challeng:0,chang:0,choos:4,cma:[5,6],cmae:4,cmaesoptim:4,code:0,collect:[2,6],comp:4,compon:0,concaten:2,conjuct:0,conjunct:0,connect:1,consist:[],constraint:[],construct:1,content:0,control:[0,6,7],copyright:[],correspond:8,cost:6,could:[4,5,6],coupl:0,covari:[0,7],craft:[],creat:3,cross:[0,7],current:[0,2,4,5,8],current_act:[6,8],current_ob:0,current_st:[2,4,6,8],custom:3,data:[0,2,5],deep:[],def_funct:2,default_inverse_transform_target:[],default_transform_target:[],defin:[1,2,3,4,5,6,8],delta:2,depict:0,deriv:0,despit:0,determin:4,determinist:[0,7],deterministicdynamicsfunctionbas:[1,2],deterministicdynamicsfunctionbaseclass:6,deterministicmlp:1,deterministictrajectoryevalu:8,deviat:2,dict:6,differ:[0,4,6],dim:[1,4,5,8],dim_:[1,4,8],dim_o:[2,4,8],dim_u:[1,2,4,5,8],dimens:[1,2,4,8],direct:4,directori:[2,6],done:[],driven:[],dtype:4,dynam:[0,4,5,6,7,8],dynamics_funct:[0,1,2,6],dynamics_handl:2,dynamics_trainer_algo:[],each:[1,2,4,5,6,8],eager:2,earlier:4,edu:4,effici:0,element:2,elit:4,email:[],enabl:0,end:1,energi:4,engin:[],entropi:[0,7],env:[0,6,7],env_class:3,env_nam:3,enviorn:[3,6],environ:[0,5,6,7],environment_util:[3,6],environment_wrapp:[],environmentwrapp:[0,3],episod:[2,4,5,6,8],epoch:[2,6],epsilon:4,equivil:[],estim:6,etc:[2,6],eth:0,ethz:[],evalu:[0,4,7],evaluate_next_reward:8,evaluatorbas:8,evaluatorbaseclass:4,everytim:[2,6],evolutionari:[0,7],exampl:0,execut:5,expand_dim:0,expected_ob:0,expected_output:1,expected_reward:0,experiment:[],explor:[5,6],exploration_nois:[4,5,6],face:0,fail:0,fals:[2,5,6],far:[1,5],feasibl:[],feasible_action_func:[],feasible_state_func:[],featur:[],figur:0,file:6,finn:[],first:[4,5,6],float32:[1,2,4,5,6,8],follow:0,forego:[],form:6,forward:0,found:[],fraction:4,framework:0,free:[0,7],from:[2,4,5,6,8],fulli:1,func:6,further:6,gamma:4,gener:0,generaliz:0,get:2,get_loss:1,get_validation_loss:1,gew:4,given:4,global:4,gradient:[],ground:1,group:[],guess:4,gym:[0,3],h_sigma:4,hand:0,handl:6,handler:[0,5,6,7,8],has:[2,6],have:4,heavili:[],help:6,henc:0,here:0,high:0,hold:6,homework:4,horizon:[4,6,8],how:[2,4,6,8],http:4,hw3:4,ieee:4,ieeexplor:4,implement:0,impress:0,includ:[],index:0,induct:[],inf:[],info:0,inform:[0,7],initi:[1,2,4,5,6,8],initial_polici:6,initial_velocity_fract:4,input:[1,2,4,8],input_dim:1,instanc:0,instanti:3,int32:[2,4,5,6,8],integr:[0,7],interact:[0,6],intergr:4,intern:8,interpret:0,invers:2,inverse_transform_actions_func:[],inverse_transform_states_func:[],inverse_transform_targets_func:2,iter:[2,4,6],iteract:6,its:[4,6,8],itself:2,jhuapl:4,jsp:4,kept:4,kera:[2,6],known:4,lab:0,lamda:4,larg:0,las:[],layer:1,learn:[0,2,5,6],learn_dynamics_from_random:6,learn_dynamics_iteratively_w_mpc:6,learning_r:[2,6],length:6,level:0,limit:0,linear:1,list:[2,3,5,6],load:2,load_saved_model:2,local:4,log:[0,2,5,6],log_dir:2,log_path:[0,6],log_result:5,lookahead:[4,6,8],loss:1,low:0,lower:[4,5],make:[3,6],make_custom_gym_env:3,make_mpc_polici:[0,6],make_standard_gym_env:[0,3],maml:[],mani:[4,6,8],manner:[],matrix:[0,7],max:4,max_iter:4,maximimum:4,mdp:2,mean:4,meta:[],method:[0,7],minor:0,mit:4,mlp:[0,7],mode:1,model:[0,2,4,6,7],modelbasedbasepolici:[5,6],modelfreebasepolici:[5,6],modul:0,more:4,mpc:[0,6,7],mpc_control:0,mpc_polici:6,mpcpolici:5,mujoco:[0,3],mujoco_env:[3,6],mujocoenv:[3,6],my_runn:0,name:[0,1,3,4,5,6,8],network:[1,2,6],neural:[2,6],neuralmpc:[],new_mean:4,next:[1,2,4,5,8],next_observ:5,next_stat:[2,4,6,8],nn_optim:[2,6],nois:[4,5,6],noise_paramet:4,non:[1,2],none:[0,1,2,5,6,8],normal:[2,6],normalized_states_devi:2,note:4,num_ag:4,num_elit:4,num_of_ag:[0,2,3,4,6,8],number:[2,3,4,5,6],number_of_ag:[0,5],number_of_initial_rollout:6,number_of_refinement_step:6,number_of_rollout:6,number_of_rollouts_for_refin:6,numpi:4,oahm:[],object:[2,3,5,6],observ:[2,4,5,6,8],observations_to_state_func:[],observations_trajectori:2,obtain:[],often:2,old_mean:4,old_stat:2,one:[3,4,6,8],onli:3,optim:[0,2,5,6,7,8],optimizer_nam:[0,5,6],optimizer_v2:[2,6],optimizerbas:4,optimizerbaseclass:[5,6],optimz:[4,6],option:0,org:4,ossama:[],other:[0,1],our:[],out:4,output:[1,2],output_dim:1,overal:0,overfit:0,own:8,packag:0,page:[],parallel:[2,3,4,5,6],parallel_env:0,paramet:[1,2,3,4,5,6,8],parellel:3,part:0,particl:[0,7],particular:[],pass:[],passs:[],path:[0,6,7],pdf:4,pendulum:0,pendulum_actions_reward_funct:0,pendulum_state_reward_funct:0,pendulumtruemodel:0,perform:[0,4,6],perform_rollout:6,perturb:[0,7],pi2:[0,5,6],pi2optim:4,plan:[4,6,8],planning_horizon:[0,4,6,8],polici:[0,6,7],popul:[4,8],population_s:4,posit:4,possibl:[4,6],postprocess:[2,5],pre:[],predict:[0,1,2,4,7,8],predict_next_st:8,prepocess:2,preprocess:[2,5,8],previou:2,problem:0,process:[2,6],process_input:2,process_state_output:2,progress:[],project:0,promp:[],propens:0,prototyp:[6,8],proven:[],provid:[0,5,6],proxim:[],pso:[5,6],psooptim:4,python:[2,6],random:[0,6,7],random_se:[0,3],randompolici:5,randomsearch:[5,6],randomsearchoptim:4,rang:0,rate:[2,6],real:[],receiv:[2,5],recent:[],record:6,record_file_path:6,record_rollout:6,recording_env:3,refer:[2,3,5,6],refin:[2,4,6],refinement_polici:[],refinemnet:6,reinforc:[],rel:2,reli:[],relianc:[],render:0,repons:2,repositori:0,research:0,reserv:[],reset:[0,4,5,6],respons:[3,4],result:[0,2,5,6],resulting_act:4,revers:[],reward:[0,2,4,5,6,8],reward_sum:6,rewards_of_next_st:[4,5],rewards_trajectori:2,right:[],robot:0,rollout:[2,6],rothfuss:[],run:[1,2,3,4,5,6],runner:[0,2,4,5,7],runnner:6,s_t:1,sampl:[0,6],save:[0,2,6],save_model_frequ:2,saved_model_dir:2,scalar:1,search:[],second:6,seed:3,seemingli:0,seen:0,sequenc:[4,5,6,8],shape:[2,4,5],shedivat:[],shoot:[0,7],should:[1,2,4,5,6],sigma:4,simulatan:4,simultan:[0,7],singl:[3,6],single_env:0,size:[2,4,6],solut:4,some:4,sourc:[1,2,3,4,5,6,8],space:[2,4,8],spall_stochastic_optim:4,specif:[],specifi:[3,4,6],split:[2,6],spsa:[4,5,6],spsaoptim:4,stack:1,stadi:[],stamp:4,standard:3,start:[6,8],state:[1,2,4,5,6,8],state_reward_funct:[0,6,8],state_to_observations_func:[],statist:2,step:[0,4,5,6,8],still:0,stochast:[0,7],store:2,str:[5,6],strategi:[0,7],string:[1,2,3,4,6,8],structur:0,subprocvecenv:[3,6],success:[],suggest:0,summari:[2,5],swarm:[0,7],switch_optim:5,system:[0,2,4,5,6,8],system_dynam:5,system_dynamics_handl:[5,6,8],systemdynam:5,systemdynamicshandl:[2,5,6,8],take:[2,3,6],taken:6,target:[2,6],task:[0,6],task_horizon:6,tend:0,tensor:4,tensorboard:[0,5],tensorflow:[2,5,6],tensorflow_writ:[],tf_func_nam:[6,8],tf_function:[2,6,8],tf_neuralmpc:[1,2,3,4,5,6,8],tf_writer:[2,5],than:4,thei:0,them:2,theoret:[0,7],theortic:4,thi:[0,1,2,3,4,5,6,8],threshold:4,tile:0,time:[4,5],time_step:[4,8],timestep:[4,5,8],top:1,tradit:3,train:[0,1,2,6],train_loss:1,trainabl:2,trainer:8,trainerbaseclass:[],traj_ac:6,traj_ob:6,traj_rew:6,trajectori:[0,4,7],trajectory_evalu:[4,8],transform:2,transform_actions_func:[],transform_states_func:[],transform_targets_func:2,true_model:[0,2,6],truth:1,tuft:4,two:6,type:[1,2,5,6,8],underli:0,understand:0,updat:[],upper:[4,5],use:[2,3,4,6],used:[0,1,2,3,4,5,6,8],uses:[0,6],using:[4,5,6,8],util:[0,7],valid:[1,2,6],validation_loss:1,validation_split:[2,6],valu:1,variou:[],vector:6,veloc:4,video:6,weak:[2,3,5,6],web:4,weight:4,well:[2,3,6],where:[2,6,8],whether:1,which:[0,1,2,3,5,6],whole:1,wrapper:[0,7],writer:[2,5],written:0,www:4,www_fall_2003:4,xdim_:2,xdim_u:2,zurich:0},titles:["Welcome to TF-NeuralMPC's documentation!","Dynamics Functions","Dynamics Handlers","Environment Utils","Optimizers","Policies","Runners","TF-NeuralMPC","Trajectory Evaluators"],titleterms:{"class":8,"function":1,adapt:4,approxim:4,base:[1,4,5,8],baselin:[],cem:[],cma_e:[],control:5,covari:4,cross:4,custom:[],determinist:[1,8],deterministic_dynamics_function_bas:[],deterministic_mlp:[],deterministic_model:[],document:0,dynam:[1,2],dynamics_funct:[],dynamics_handl:[],entropi:4,env:3,environ:3,evalu:8,evaluator_baseclass:[],evolutionari:4,free:5,handler:2,indic:0,inform:4,integr:4,interfac:[],matrix:4,method:4,mlp:1,model:5,model_based_base_polici:[],model_free_base_polici:[],mpc:4,mpc_polici:[],neuralmpc:[0,7],optim:4,optimizer_bas:[],particl:4,path:4,perturb:4,pi2:[],polici:5,predict:5,pso:[],random:[4,5],random_polici:[],random_search:[],randomsearch:[],runner:6,search:[],shoot:4,simultan:4,spsa:[],standard:[],stochast:4,strategi:4,subprocess:[],swarm:4,system_dynamics_handl:[],tabl:0,tf_neuralmpc:[],theoret:4,trajectori:8,trajectory_evalu:[],util:3,welcom:0,wrapper:3}})