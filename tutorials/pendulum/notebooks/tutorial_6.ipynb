{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a parallel environment for the pendulum environment and then learn the dynamics model\n",
    "from random rollouts initially and use MPC to collect more samples and refine the model in an iterative fashion.\n",
    "Then load the saved model and using it to learn further or just for control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_neuralmpc.environment_utils import EnvironmentWrapper\n",
    "import logging\n",
    "from tf_neuralmpc.dynamics_functions import DeterministicMLP\n",
    "from tf_neuralmpc.dynamics_handlers.system_dynamics_handler import SystemDynamicsHandler\n",
    "from tf_neuralmpc.examples.cost_funcs import pendulum_actions_reward_function, pendulum_state_reward_function\n",
    "from tf_neuralmpc import Runner\n",
    "import tensorflow as tf\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_agents = 2\n",
    "log_path = './tutorial_6'\n",
    "single_env, parallel_env = EnvironmentWrapper.make_standard_gym_env(\"Pendulum-v0\", random_seed=0,\n",
    "                                                                    num_of_agents=number_of_agents)\n",
    "my_runner = Runner(env=[single_env, parallel_env],\n",
    "                   log_path=log_path,\n",
    "                   num_of_agents=number_of_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dynamics model architecture now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = single_env.observation_space.shape[0]\n",
    "input_size = single_env.action_space.shape[0]\n",
    "dynamics_function = DeterministicMLP()\n",
    "dynamics_function.add_layer(state_size + input_size,\n",
    "                            32, activation_function=tf.math.tanh)\n",
    "dynamics_function.add_layer(32, 32, activation_function=tf.math.tanh)\n",
    "dynamics_function.add_layer(32, 32, activation_function=tf.math.tanh)\n",
    "dynamics_function.add_layer(32, state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now learn the dynamics model using the random rollouts and then collect more samples from the MPC controller for finetuning the model further. Note: real number of rollouts in totoal is eqaul to number_of_agents*number_of_rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started collecting samples for rollouts\n",
      "INFO:root:Average action selection time: 0.00036170363426208497\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.00027430057525634766\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002607285976409912\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002713155746459961\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0003305566310882568\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002593553066253662\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.000253450870513916\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0003247642517089844\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002527165412902832\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002457737922668457\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.00025023102760314943\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002751326560974121\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002895605564117432\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002585387229919434\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.00023958802223205566\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002742469310760498\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0002946949005126953\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.00032986164093017577\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.0003464245796203613\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.00023670196533203124\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Finished collecting samples for rollout\n",
      "INFO:root:Started the system training\n",
      "INFO:root:Saving the model now....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ossamaahmed/.virtualenvs/model_based_rl/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ossamaahmed/.virtualenvs/model_based_rl/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tutorial_6/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tutorial_6/saved_model/assets\n",
      "INFO:root:Trained initial system model\n",
      "INFO:root:Started collecting samples for rollouts\n",
      "INFO:root:Average action selection time: 0.09817265152931214\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.09045002818107604\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Finished collecting samples for rollout\n",
      "INFO:root:Started the system training\n",
      "INFO:root:Saving the model now....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tutorial_6/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tutorial_6/saved_model/assets\n",
      "INFO:root:Started collecting samples for rollouts\n",
      "INFO:root:Average action selection time: 0.0816757893562317\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.08248288035392762\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Finished collecting samples for rollout\n",
      "INFO:root:Started the system training\n",
      "INFO:root:Saving the model now....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tutorial_6/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tutorial_6/saved_model/assets\n",
      "INFO:root:Started collecting samples for rollouts\n",
      "INFO:root:Average action selection time: 0.08029894351959228\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Average action selection time: 0.08095643043518067\n",
      "INFO:root:Rollout length: 200\n",
      "INFO:root:Finished collecting samples for rollout\n"
     ]
    }
   ],
   "source": [
    "system_dynamics_handler, mpc_policy = my_runner.learn_dynamics_iteratively_w_mpc(number_of_initial_rollouts=20,\n",
    "                                                                                 number_of_rollouts_for_refinement=2,\n",
    "                                                                                 number_of_refinement_steps=2,\n",
    "                                                                                 dynamics_function=dynamics_function,\n",
    "                                                                                 task_horizon=200,\n",
    "                                                                                 planning_horizon=40,\n",
    "                                                                                 state_reward_function=pendulum_state_reward_function,\n",
    "                                                                                 actions_reward_function=pendulum_actions_reward_function,\n",
    "                                                                                 optimizer_name='PI2',\n",
    "                                                                                 exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 85723), started 0:10:19 ago. (Use '!kill 85723' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c6b4d6aa2b2a2ed3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c6b4d6aa2b2a2ed3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_runner.record_rollout(horizon=500, policy=mpc_policy,\n",
    "                         record_file_path=log_path+'/episode_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the saved model and create another policy using the loaded dynamics model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading the saved model now....\n"
     ]
    }
   ],
   "source": [
    "new_log_path = './tutorial_6_loaded_model'\n",
    "new_systems_dynamics_handler = SystemDynamicsHandler(dim_O=state_size,\n",
    "                                                     dim_U=input_size,\n",
    "                                                     num_of_agents=1,\n",
    "                                                     log_dir=new_log_path,\n",
    "                                                     saved_model_dir=log_path+'/saved_model',\n",
    "                                                     load_saved_model=True)\n",
    "new_mpc_controller = my_runner.make_mpc_policy(system_dynamics_handler=new_systems_dynamics_handler,\n",
    "                                               state_reward_function=pendulum_state_reward_function,\n",
    "                                               actions_reward_function=pendulum_actions_reward_function,\n",
    "                                               planning_horizon=50,\n",
    "                                               optimizer_name='PI2',\n",
    "                                               true_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_runner.record_rollout(horizon=500, policy=new_mpc_controller,\n",
    "                         record_file_path=new_log_path+'/episode_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
