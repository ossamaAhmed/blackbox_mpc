

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="python" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="python" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to TF-NeuralMPC&#39;s documentation! &mdash; tf_neuralmpc 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="TF-NeuralMPC" href="modules/tf_neuralmpc.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> tf_neuralmpc
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules/tf_neuralmpc.html">TF-NeuralMPC</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">tf_neuralmpc</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to TF-NeuralMPC's documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="welcome-to-tf-neuralmpc-s-documentation">
<h1>Welcome to TF-NeuralMPC's documentation!<a class="headerlink" href="#welcome-to-tf-neuralmpc-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>Despite the impressive results of current RL algorithms, they still face important limitations, namely, low
sample efficiency and a propensity not to generalize to seemingly minor changes in the task. These challenges
suggest that large capacity model-free RL models tend to overfit to the abundant data on which they are trained
on and hence fail to learn an abstract, interpretable, and generalizable understanding of the underlying problem.
Model-based RL on the other hand learns a forward model of the environment and then uses it in conjunction
with a model-predictive controller for instance to control the agent/ robot, the advantage here is that the learned
forward model can be used in performing different tasks.</p>
<a class="reference internal image-reference" href="_images/mpc.png"><img alt="_images/mpc.png" src="_images/mpc.png" style="width: 600px;" /></a>
<p>The code repository provides a framework of different derivative-free optimizers which can be used in
conjuction with a model predictive controller and a learned dynamics model to control an agent in a
mujoco or a gym environment.</p>
<p>The code was written as part of a research project at the Learning and Adaptive Systems Lab &#64;ETH Zurich.
Overall, the aim of this package is to be enable performing optimal control with  model-predictive control
on any mujoco or gym environment in couple of steps. An example can be seen below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">number_of_agents</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">single_env</span><span class="p">,</span> <span class="n">parallel_env</span> <span class="o">=</span> <span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                 <span class="n">num_of_agents</span><span class="o">=</span><span class="n">number_of_agents</span><span class="p">)</span>
<span class="n">my_runner</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="p">[</span><span class="n">single_env</span><span class="p">,</span> <span class="n">parallel_env</span><span class="p">],</span>
                <span class="n">log_path</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">num_of_agents</span><span class="o">=</span><span class="n">number_of_agents</span><span class="p">)</span>
<span class="n">mpc_controller</span> <span class="o">=</span> <span class="n">my_runner</span><span class="o">.</span><span class="n">make_mpc_policy</span><span class="p">(</span><span class="n">dynamics_function</span><span class="o">=</span><span class="n">PendulumTrueModel</span><span class="p">(),</span>
                                        <span class="n">state_reward_function</span><span class="o">=</span><span class="n">pendulum_state_reward_function</span><span class="p">,</span>
                                        <span class="n">actions_reward_function</span><span class="o">=</span><span class="n">pendulum_actions_reward_function</span><span class="p">,</span>
                                        <span class="n">planning_horizon</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                                        <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;PI2&#39;</span><span class="p">,</span>
                                        <span class="n">true_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">single_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">current_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">current_obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                   <span class="p">(</span><span class="n">number_of_agents</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
 <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_controller</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
 <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">single_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
 <span class="n">current_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">current_obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                       <span class="p">(</span><span class="n">number_of_agents</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
 <span class="n">single_env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
<p>This high level functionalities of the package are implemented in the Runner class and the overall structure and interaction
of the code components is depicted in the following figure:</p>
<a class="reference internal image-reference" href="_images/flowchart.png"><img alt="_images/flowchart.png" src="_images/flowchart.png" style="width: 600px;" /></a>
<p>There is the option of logging the results in tensorboard and saving the dynamics model trained as the following:</p>
<a class="reference internal image-reference" href="_images/results.png"><img alt="_images/results.png" src="_images/results.png" style="width: 1000px;" /></a>
<a class="reference internal image-reference" href="_images/uncertainity.png"><img alt="_images/uncertainity.png" src="_images/uncertainity.png" style="width: 1000px;" /></a>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules/tf_neuralmpc.html">TF-NeuralMPC</a><ul>
<li class="toctree-l2"><a class="reference internal" href="modules/runner/runner.html">Runners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/runner/runner.html#runner">Runner</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modules/optimizers/optimizers.html">Optimizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#optimizer-base">Optimizer Base</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#cross-entropy-method">Cross Entropy Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#random-shooting">Random Shooting</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#covariance-matrix-adaptation-evolutionary-strategy">Covariance Matrix Adaptation Evolutionary-Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#particle-swarm-optimizer">Particle Swarm Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#path-integral-information-theoretic-mpc">Path Integral (Information Theoretic MPC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/optimizers/optimizers.html#simultaneous-perturbation-stochastic-approximation-optimizer">Simultaneous Perturbation Stochastic Approximation Optimizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modules/policies/policies.html">Policies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/policies/policies.html#model-based-base-policy">Model Based Base Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/policies/policies.html#model-predictive-control-policy">Model Predictive Control Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/policies/policies.html#model-free-base-policy">Model Free Base Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/policies/policies.html#random-policy">Random Policy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modules/dynamics_functions/dynamics_functions.html">Dynamics Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/dynamics_functions/dynamics_functions.html#deterministic-dynamics-base-function">Deterministic Dynamics Base Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/dynamics_functions/dynamics_functions.html#deterministic-dynamics-mlp-function">Deterministic Dynamics MLP Function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modules/dynamics_handlers/dynamics_handlers.html">Dynamics Handlers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/dynamics_handlers/dynamics_handlers.html#id1">Dynamics Handlers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modules/trajectory_evaluators/trajectory_evaluators.html">Trajectory Evaluators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/trajectory_evaluators/trajectory_evaluators.html#evaluator-base-class">Evaluator Base Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="modules/trajectory_evaluators/trajectory_evaluators.html#deterministic-evaluator">Deterministic Evaluator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modules/environment_utils/environment_utils.html">Environment Utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="modules/environment_utils/environment_utils.html#env-wrapper">Env Wrapper</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="modules/tf_neuralmpc.html" class="btn btn-neutral float-right" title="TF-NeuralMPC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ossama Ahmed and Jonas Ruthfuss

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>